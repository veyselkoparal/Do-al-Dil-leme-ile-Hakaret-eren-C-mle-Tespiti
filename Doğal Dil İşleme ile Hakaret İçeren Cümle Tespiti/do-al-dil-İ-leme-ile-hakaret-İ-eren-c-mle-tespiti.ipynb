{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdf553900a4a7463e57cc4c69ac960161f2b553c"
   },
   "source": [
    "**GİRİŞ**\n",
    "\n",
    "* Bu projemizde Turkish cyberbullying datasını kullanarak; Türkçe hakaret içeren cümleleri tespit edebilen bir model eğiteceğiz. Bu model içinde CountVectorizer ve TFIDF Vectorizer'ları ile tanışıp, Pipeline yapısını kullanmayı öğreneceğiz. En son olarak da modelimizi Joblib ile '.pkl' uzantılı bir dosyaya kaydedip, daha sonraki kullanımlarımızda yalnızca bu dosyayı çağırarak işlem yapacağız.\n",
    "\n",
    "* Benim ilk NLP ve ilk Türkçe kernel'ım olacak bu çalışma. Dolayısıyla çok heyecalıyım. O zaman başlayalım!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['turkish cyberbullying.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/turkish cyberbullying.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c5b74cc1d09c2e45d2d57cfaeeec22510a5c5af6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>cyberbullying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rabbim kalan ömrünü geçen ömründen hayırlı eyl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir ateist olarak bu resmi gördükçe gözyaşları...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oo süpersin azıcık bize de bulaşsa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bende biliyorum benden bı bok olmicak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nerdesin len tirrek</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  cyberbullying\n",
       "0  rabbim kalan ömrünü geçen ömründen hayırlı eyl...              0\n",
       "1  bir ateist olarak bu resmi gördükçe gözyaşları...              0\n",
       "2                 oo süpersin azıcık bize de bulaşsa              0\n",
       "3              bende biliyorum benden bı bok olmicak              1\n",
       "4                                nerdesin len tirrek              1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d45aee18234b49a83d09bf3690274f2089901f47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>cyberbullying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>çikolata kakaodan yapılıyor kakao ise ağacın t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>sen çok kafana takıyorsun takıyorum kardeşim k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>birinden kaçtin ama diğerinden kaçamadınbu ülk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>düğün var diye mutlu olan var mı allah aşkına ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>vefa ile ne alaka vefanın anlamını biliyor mu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>bismillahirahmanirahim yaallah hayırlı sabahla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>jskdpdğeff deli amk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>nasıl bir yavşak serefsizsiniz hepiniz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>shipshipshiphayırlı olsun</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>çookk tatlısın çok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message  cyberbullying\n",
       "2991  çikolata kakaodan yapılıyor kakao ise ağacın t...              0\n",
       "2992  sen çok kafana takıyorsun takıyorum kardeşim k...              1\n",
       "2993  birinden kaçtin ama diğerinden kaçamadınbu ülk...              0\n",
       "2994  düğün var diye mutlu olan var mı allah aşkına ...              0\n",
       "2995  vefa ile ne alaka vefanın anlamını biliyor mu ...              0\n",
       "2996  bismillahirahmanirahim yaallah hayırlı sabahla...              0\n",
       "2997                                jskdpdğeff deli amk              1\n",
       "2998             nasıl bir yavşak serefsizsiniz hepiniz              1\n",
       "2999                          shipshipshiphayırlı olsun              0\n",
       "3000                                 çookk tatlısın çok              0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "348d61113e6f02262c3f5a2da5d6fdd517362d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3001 entries, 0 to 3000\n",
      "Data columns (total 2 columns):\n",
      "message          3001 non-null object\n",
      "cyberbullying    3001 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f6fb2629aed1e153175732601543a9eb1087b304"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyberbullying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cyberbullying\n",
       "count    3001.000000\n",
       "mean        0.500833\n",
       "std         0.500083\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "cbb3b07257fb8764f549931c1decf12ff4355bda"
   },
   "outputs": [],
   "source": [
    "y = data.cyberbullying.values\n",
    "x = data.message.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4843e826cb2999091fd1c2cd844b239e634dab8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rabbim kalan ömrünü geçen ömründen hayırlı eylesin'\n",
      " 'bir ateist olarak bu resmi gördükçe gözyaşlarıma mani olamıyorum'\n",
      " 'oo süpersin azıcık bize de bulaşsa' ...\n",
      " 'nasıl bir yavşak serefsizsiniz hepiniz' 'shipshipshiphayırlı olsun'\n",
      " 'çookk tatlısın çok']\n",
      "3001\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "361ef141aac6649a1ba8b3ff11dcccc83afb87e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3001,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8c3967ad8a48728a3f9fb5908ade42353681d89"
   },
   "source": [
    "Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "a8ac9c9cadb05ddbf24d0c4e9d7ddfa75226b2e9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1c275e58c32a31ff4b3e16fb69dc349baa92138c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250\n",
      "751\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "70e899fbb2aa64c632591a30c07a6d9c00d46f93"
   },
   "outputs": [],
   "source": [
    "veri = x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "50ef7dca6cc0be5b1e4bd96c93c683510be1a9a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3001,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veri.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "081fa9d578784f7d1aabaee78a8c5de31efb0b5f"
   },
   "source": [
    "Türkçe'de bulunan '**Stopwords**'(gereksiz kelimeler)'den kurtulmamız gerekiyor çünkü bu kelimeler classification yaparken işimize yaramayacak kelimeler. O nedenle bu kelimelerden kurtulalım!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "624718ce53800f67bc13a3d5848908c2113bf82b"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"turkish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21086e2a59fd1c8d9ef80af8a948d6f444214290"
   },
   "source": [
    "**CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f48f659f12ad75c2a746e21a40eec2c9f558527b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), preprocessor=None,\n",
       "        stop_words=['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=5,stop_words=stop,ngram_range=(1,3))\n",
    "vectorizer.fit(veri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d0f19863ec5afdb6ffe5cea0c17be1f5f20f8c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<3001x795 sparse matrix of type '<class 'numpy.int64'>'\\n\\twith 11001 stored elements in Compressed Sparse Row format>\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW = vectorizer.transform(veri)\n",
    "repr(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "4771f7ecbbee3363ad6ff74dddecabed3f38e689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ile 110 arasındaki değerler:\n",
      "['ben', 'bence', 'bende', 'benden', 'beni', 'benim', 'beraber', 'berduş', 'beri', 'bey']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"100 ile 110 arasındaki değerler:\\n{}\".format(feature_names[100:110]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "098a758bd9f753200c7a5f692808c10e326ec649"
   },
   "source": [
    "**TFIDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "08b8328bafea7f2198645f2a6e7607d11b5c4ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies using min_df=1 and n_gram=(1, 1) with highest tfidf: \n",
      "['sınav' 'uzat' 'gittin' 'sıffır' 'hödük' 'vay' 'tarzdır' 'hee' 'yalayın'\n",
      " 'styling' 'güzelsin' 'tatlısın' 'tırrek' 'andaval' 'sisko' 'şişko'\n",
      " '09052018' 'diyarbakır' 'tatlı' 'yobaz']\n",
      "The number of vocabularies: 11458\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=1 and n_gram=(1, 2) with highest tfidf: \n",
      "['güzelsin kız' 'güzelsin çocuk' 'herkes gerizeka' 'uzat' 'güzelsin sen'\n",
      " 'güzelsin güzel' 'ulan amk' 'hee' 'yalayın' 'diyarbakır' 'andaval'\n",
      " 'sisko' 'tırrek' 'tatlısın' '09052018' 'styling' 'yobaz' 'tatlı' 'şişko'\n",
      " 'güzelsin']\n",
      "The number of vocabularies: 32321\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=1 and n_gram=(1, 3) with highest tfidf: \n",
      "['harikasın sen' 'angut var' 'embesil seni' 'güzelsin çocuk'\n",
      " 'güzelsin kız' 'herkes gerizeka' 'güzelsin sen' 'güzelsin güzel'\n",
      " 'ulan amk' 'güzelsin' '09052018' 'diyarbakır' 'styling' 'tatlı'\n",
      " 'tatlısın' 'sisko' 'yobaz' 'şişko' 'andaval' 'tırrek']\n",
      "The number of vocabularies: 51144\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=1 and n_gram=(2, 3) with highest tfidf: \n",
      "['orospu catelyn' 'huzurlu sabahlar' 'tatlisin yaa' 'sevimsiz pezevenk'\n",
      " 'beyinsiz tudor' 'tatilse tatil' 'yarak nasilsin'\n",
      " 'flamingoları öldürmeyin' 'enanlamlıiyiki ulasastepe' 'çookk tatlısın'\n",
      " 'güzelsin army' 'ağzına sıçarım' 'siktir puşt' 'hakkınızdı tebrikler'\n",
      " 'irkçı pislikler' 'öküz bakıyorsunuz' 'hayırlı cumalar' 'biraz müzik'\n",
      " 'titrek serap' 'kinci pezevenk']\n",
      "The number of vocabularies: 39686\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'kutlu olsun' 'tam bir' 'hayırlı olsun'\n",
      " 'orospu çocukları' 'siktir git' 'bir şarkı' 'bir gün' 'günün kutlu'\n",
      " 'hayırlı sabahlar' 'sen kimsin' 'lanet olsun' 'günün kutlu olsun'\n",
      " 'değil mi' 'amına koyduğum' 'danla bilic' 'hadi hayırlısı'\n",
      " 'doğum günün kutlu' 'düğün var' 'doğum günün']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(1, 1) with highest tfidf: \n",
      "['piç' 'izmir' 'tırrek' 'sapık' 'döl' 'pislikler' 'kaldı' 'dümbük' 'dünya'\n",
      " 'dünyada' 'dürzü' 'düğün' 'başlasın' 'pislik' 'musmutlu' 'kadar'\n",
      " 'zampara' 'ender' 'mutluyum' 'hayirlisi']\n",
      "The number of vocabularies: 2887\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(1, 2) with highest tfidf: \n",
      "['dangalak' 'dallama' 'yazik' 'dünyada' 'başarılı' 'kubilay' 'yaz'\n",
      " 'önemli' 'sen' 'döl' 'dünya' 'güzelsin' 'barzo' 'dümbük' 'konseri'\n",
      " 'kocasının' 'bakın' 'cüce' 'ol' 'teşekkür']\n",
      "The number of vocabularies: 3714\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(1, 3) with highest tfidf: \n",
      "['soğuk' 'olsun' 'başarılı' 'on' 'orospu' 'başlasın' 'sorular' 'be' 'para'\n",
      " 'berduş' 'son' 'solmasın' 'düğün' 'sokarım' 'dürzü' 'dünyada'\n",
      " 'bilgisayar' 'bir' 'dangalak' 'kaya']\n",
      "The number of vocabularies: 4007\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(2, 3) with highest tfidf: \n",
      "['hayırlısı olsun' 'kadar güzel' 'kabul etti' 'hiçbir zaman'\n",
      " 'hocasıyla öğrencisiyle' 'huzurlu sabahlar' 'hıncal uluç' 'ilk yarı'\n",
      " 'insan hakları' 'insanlar var' 'insanları üzmeyin' 'ismail kahraman'\n",
      " 'iyi bir' 'iyi kötü' 'iyi sen' 'işi gücü' 'işin var'\n",
      " 'işte cerrahpaşaduruşu' 'kadar andaval' 'kadar güzelsin']\n",
      "The number of vocabularies: 1120\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'kutlu olsun' 'tam bir' 'hayırlı olsun'\n",
      " 'orospu çocukları' 'siktir git' 'bir şarkı' 'hayırlı sabahlar'\n",
      " 'günün kutlu' 'sen kimsin' 'bir gün' 'lanet olsun' 'günün kutlu olsun'\n",
      " 'değil mi' 'doğum günün kutlu' 'amına koyduğum' 'danla bilic' 'düğün var'\n",
      " 'hadi hayırlısı' 'doğum günün']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(1, 1) with highest tfidf: \n",
      "['güzelsin' 'günaydın' 'göt' 'sorular' 'hayırlısı' 'gurur' 'gerizeka'\n",
      " 'geldi' 'sahip' 'gece' 'gavur' 'galatasaray' 'fahişe' 'yarak' 'ey' 'evde'\n",
      " 'et' 'ender' 'sikik' 'yaz']\n",
      "The number of vocabularies: 1556\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(1, 2) with highest tfidf: \n",
      "['hala' 'hali' 'bilgisayar' 'sürtük' 'harikasın' 'süpersin' 'berduş'\n",
      " 'hayır' 'hayırlı' 'hayırlısı' 'hazan' 'ben' 'soğuk' 'be' 'soytarı'\n",
      " 'yobaz' 'yok' 'başarılı' 'günaydın' 'olmuyor']\n",
      "The number of vocabularies: 1729\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(1, 3) with highest tfidf: \n",
      "['yaz' 'bücür' 'yazik' 'böyle' 'soğuk' 'soytarı' 'sorular' 'hocam' 'bok'\n",
      " 'hödük' 'son' 'sol' 'birde' 'sokarım' 'hırtapoz' 'sizin' 'ibne' 'bir'\n",
      " 'hazan' 'olsa']\n",
      "The number of vocabularies: 1751\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(2, 3) with highest tfidf: \n",
      "['defol git' 'ders çalışmak' 'devam ediyor' 'devam et' 'değil mi'\n",
      " 'dikkat edin' 'düğün sezonu' 'düğün var' 'kadar güzelsin'\n",
      " 'elimde telefon' 'fatih terim' 'gediz deltası' 'gerek yok' 'geri zekalı'\n",
      " 'gerizeka bir' 'gerizeka tudor' 'git lan' 'gün boyu' 'embesil sen'\n",
      " 'herkese hayırlı']\n",
      "The number of vocabularies: 195\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'kutlu olsun' 'tam bir' 'hayırlı olsun'\n",
      " 'orospu çocukları' 'siktir git' 'bir şarkı' 'bir gün' 'hayırlı sabahlar'\n",
      " 'günün kutlu' 'sen kimsin' 'günün kutlu olsun' 'değil mi' 'lanet olsun'\n",
      " 'düğün var' 'doğum günün kutlu' 'amına koyduğum' 'danla bilic'\n",
      " 'doğum günün' 'hadi hayırlısı']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(1, 1) with highest tfidf: \n",
      "['tatlısın' 'iyiki' 'iyi' 'tebrik' 'herkesi' 'tebrikler' 'insan' 'telefon'\n",
      " 'ilkbahar' 'teşekkür' 'teşekkürler' 'tirrek' 'ibne' 'hırtapoz' 'hödük'\n",
      " 'türkiye' 'hocam' 'tırrek' 'insanlar' 'şırfıntı']\n",
      "The number of vocabularies: 1016\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(1, 2) with highest tfidf: \n",
      "['hafta' 'dallama' 'sol' 'sonra' 'fena' 'galatasaray' 'sıra' 'gavur'\n",
      " 'sürtük' 'gece' 'son' 'süpersin' 'geldi' 'gerizeka' 'gerzek' 'soğuk'\n",
      " 'soytarı' 'gurur' 'gel' 'şırfıntı']\n",
      "The number of vocabularies: 1096\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(1, 3) with highest tfidf: \n",
      "['dünyadaki' 'dünyada' 'türkiye' 'dünya' 'tebrik' 'dümbük' 'sahip' 'döl'\n",
      " 'dua' 'ibne' 'dombili' 'ilkbahar' 'insan' 'insanlar' 'dingil' 'puşt'\n",
      " 'değerli' 'iyi' 'tırrek' 'şırfıntı']\n",
      "The number of vocabularies: 1100\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(2, 3) with highest tfidf: \n",
      "['bir pislik' 'bir sanat' 'hayırlı akşamlar' 'şişko yanaklarımı'\n",
      " 'güzel bir' 'git lan' 'gerizeka tudor' 'gerizeka bir' 'geri zekalı'\n",
      " 'hayırlı olsun' 'gerek yok' 'değil mi' 'defol git' 'danla bilic'\n",
      " 'bundan sonra' 'bir şarkı' 'bir çocuk' 'bir sürü' 'düğün var'\n",
      " 'hadi hayırlısı']\n",
      "The number of vocabularies: 84\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'kutlu olsun' 'tam bir' 'hayırlı olsun'\n",
      " 'orospu çocukları' 'siktir git' 'bir şarkı' 'sen kimsin'\n",
      " 'hayırlı sabahlar' 'günün kutlu' 'bir gün' 'günün kutlu olsun' 'değil mi'\n",
      " 'lanet olsun' 'danla bilic' 'düğün var' 'doğum günün kutlu' 'doğum günün'\n",
      " 'amına koyduğum' 'hadi hayırlısı']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(1, 1) with highest tfidf: \n",
      "['kız' 'kültür' 'kutlu' 'kitap' 'kerhane' 'kendi' 'keko' 'kaybetmekorkusu'\n",
      " 'ilkbahar' 'karşı' 'kaldı' 'kahpe' 'kadın' 'kadro' 'kadar' 'iyi'\n",
      " 'istiyorum' 'insanlar' 'kardeşim' 'şırfıntı']\n",
      "The number of vocabularies: 759\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(1, 2) with highest tfidf: \n",
      "['dallama' 'cüce' 'sinema' 'cok' 'böyle' 'gerzek' 'sizin' 'cidden'\n",
      " 'cibiliyetsiz' 'sokarım' 'guzel' 'son' 'sonra' 'cerrahpaşaduruşu'\n",
      " 'cenabet' 'göt' 'bütün' 'bücür' 'gurur' 'şırfıntı']\n",
      "The number of vocabularies: 793\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(1, 3) with highest tfidf: \n",
      "['dallama' 'cüce' 'sinema' 'cok' 'böyle' 'gerzek' 'sizin' 'cidden'\n",
      " 'cibiliyetsiz' 'sokarım' 'guzel' 'son' 'sonra' 'cerrahpaşaduruşu'\n",
      " 'cenabet' 'göt' 'bütün' 'bücür' 'gurur' 'şırfıntı']\n",
      "The number of vocabularies: 795\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(2, 3) with highest tfidf: \n",
      "['tam bir' 'nefret ediyorum' 'kadar güzel' 'hayırlı cumalar'\n",
      " 'hayırlı olsun' 'türkiye nin' 'hadi hayırlısı' 'gerizeka tudor'\n",
      " 'gerek yok' 'düğün var' 'değil mi' 'danla bilic' 'bir şarkı' 'bir sürü'\n",
      " 'bir pislik' 'bir orospu' 'bir gün' 'bir adam' 'hayırlı sabahlar'\n",
      " 'şişko yanaklarımı']\n",
      "The number of vocabularies: 36\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'kutlu olsun' 'tam bir' 'hayırlı olsun'\n",
      " 'orospu çocukları' 'siktir git' 'bir şarkı' 'bir gün' 'sen kimsin'\n",
      " 'hayırlı sabahlar' 'günün kutlu' 'değil mi' 'lanet olsun'\n",
      " 'günün kutlu olsun' 'hadi hayırlısı' 'amına koyduğum' 'düğün var'\n",
      " 'doğum günün' 'danla bilic' 'doğum günün kutlu']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(1, 1) with highest tfidf: \n",
      "['sahip' 'sakın' 'dünyanın' 'sanane' 'başladı' 'sanat' 'kendi' 'birde'\n",
      " 'sen' 'sende' 'bir' 'seni' 'senin' 'bilgisayar' 'serefiz' 'sersem' 'keko'\n",
      " 'karşı' 'sanırım' 'şırfıntı']\n",
      "The number of vocabularies: 608\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'kadar' 'ben' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(1, 2) with highest tfidf: \n",
      "['sanane' 'sanat' 'sanırım' 'hangi' 'dünyanın' 'hala' 'sen' 'güzelsin'\n",
      " 'sende' 'günaydın' 'seni' 'senin' 'göt' 'serefiz' 'sersem' 'guzel'\n",
      " 'gurur' 'sikerim' 'hafta' 'şırfıntı']\n",
      "The number of vocabularies: 630\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'mi' 'pislik' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(1, 3) with highest tfidf: \n",
      "['rağmen' 'guzel' 'gurur' 'sabah' 'gerzek' 'sahip' 'sakın' 'sanane'\n",
      " 'sanat' 'sanırım' 'gerizeka' 'gerek' 'gelsin' 'sen' 'geldi' 'sende' 'gel'\n",
      " 'seni' 'dönek' 'şırfıntı']\n",
      "The number of vocabularies: 632\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'var' 'bi' 'senin' 'orospu' 'amk' 'seni'\n",
      " 'olsun' 'lan' 'gerizeka' 'mutluyum' 'yok' 'pislik' 'mi' 'güzel' 'andaval'\n",
      " 'değil']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(2, 3) with highest tfidf: \n",
      "['sen kimsin' 'orospu çocuğu' 'orospu çocukları' 'nefret ediyorum'\n",
      " 'lanet olsun' 'kutlu olsun' 'hayırlı sabahlar' 'hayırlı olsun'\n",
      " 'hayırlı cumalar' 'amına koyduğum' 'siktir git' 'düğün var' 'değil mi'\n",
      " 'danla bilic' 'bir şarkı' 'bir pislik' 'bir orospu' 'bir gün'\n",
      " 'hadi hayırlısı' 'tam bir']\n",
      "The number of vocabularies: 24\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'kutlu olsun' 'tam bir' 'orospu çocukları'\n",
      " 'hayırlı olsun' 'siktir git' 'bir şarkı' 'bir gün' 'sen kimsin'\n",
      " 'hayırlı sabahlar' 'günün kutlu' 'lanet olsun' 'günün kutlu olsun'\n",
      " 'değil mi' 'düğün var' 'doğum günün kutlu' 'doğum günün' 'danla bilic'\n",
      " 'hadi hayırlısı' 'amına koyduğum']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "for min_df in [1,2,3,4,5,6]:\n",
    "    for n_gram in [(1,1),(1,2),(1,3),(2,3)]:\n",
    "        tf_vectorizer = TfidfVectorizer(min_df=min_df, stop_words=stop,ngram_range=n_gram)\n",
    "        veri1 = tf_vectorizer.fit_transform(veri)\n",
    "        best = veri1.max(axis=0).toarray().ravel()\n",
    "        sort_by_tfidf = best.argsort()\n",
    "        feature_names = np.array(tf_vectorizer.get_feature_names())\n",
    "        print(\"Vocabularies using min_df={} and n_gram={} with highest tfidf: \\n{}\".format(min_df, n_gram, feature_names[sort_by_tfidf[-20:]]))\n",
    "        print(\"The number of vocabularies: {}\".format(len(tf_vectorizer.vocabulary_)))\n",
    "        sort_by_tfidf = np.argsort(tf_vectorizer.idf_)\n",
    "        print(\"Vocabularies with lowest idf:\\n{}\".format(feature_names[sort_by_tfidf[:20]]))\n",
    "        print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a3f6bda5b9f7eb617a4705b230fcfad4618b656"
   },
   "source": [
    "Modelimizi eğitirken kullanacağımız metotlarımızı import ediyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "e368bcf11756955b8a419fd967bf58ded9b3ce14"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b6dabd7bd317d587fa5f49d9effc697fcb438c25"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "60108fdd21b40a247f895e5e73a6a983e053b644"
   },
   "source": [
    "Çalışmamda Pipeline yapısı kullanacağım. Pipeline'ı bilmeyenler için => https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "5724ddae8dbb05f2648149b7474ad5092da8a719"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff2bf08f0e29bcb80358f2b9025fca41f5b8cd28"
   },
   "source": [
    "Modelde kullanacağımız metotları, vectorizerları da kullanarak tek tek tanıtmaya başlıyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "8cf05b3f70a2e44e9cc6e049281874e42c508742"
   },
   "outputs": [],
   "source": [
    "\n",
    "LinearSVC_count = Pipeline([\n",
    "    ('countvectorizer',CountVectorizer()),\n",
    "    ('LinearSVC',LinearSVC(max_iter=1000))\n",
    "])\n",
    "\n",
    "LinearSVC_tfidf = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('LinearSVC', LinearSVC(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "4be43daec5b7daad4cf85ca063a639734d164913"
   },
   "outputs": [],
   "source": [
    "Naive_count = Pipeline([\n",
    "        ('countvectorizer', CountVectorizer()),\n",
    "        ('multinomialnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "Naive_tfidf = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('multinomialnb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "0374759392bd0cdde382e59cabeb737141e5c50c"
   },
   "outputs": [],
   "source": [
    "Decision_count = Pipeline([\n",
    "        ('countvectorizer', CountVectorizer()),\n",
    "        ('decisiontreeclassifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "Decision_tfidf = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('decisiontreeclassifier', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "88015c031fb8e0ddaebcb8573ceef5bc8b70b9c4"
   },
   "outputs": [],
   "source": [
    "RandomForest_count = Pipeline([\n",
    "        ('countvectorizer', CountVectorizer()),\n",
    "        ('randomforestclassifier', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "RandomForest_tfidf = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('randomforestclassifier', RandomForestClassifier(n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5dbbe58ea4b36f64e8d47edad5cfda99e7b6f15"
   },
   "source": [
    "Şimdi parametrelerimizi tanıtalım. Metotların içinden yalnızca LinearSVC hyper parameter olarak 'C' parametresini alıyor. O nedenle LinearSVC parametrelerini ayrı, kalan 3 metotun parametrelerini ayrı alacağım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "d23e610f30931a1eabfd242cd73a084286e7ec45"
   },
   "outputs": [],
   "source": [
    "#İlk olarak bünyesinde Count Vectorizer olan parametreler.\n",
    "parameters_of_svc_count = [ \n",
    "    {\n",
    "        'LinearSVC__C': [0.01, 0.1, 1, 10, 100], \n",
    "        'countvectorizer__min_df': [1,3,5], \n",
    "        'countvectorizer__stop_words': [None, stop],\n",
    "        'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    } \n",
    "]\n",
    "\n",
    "parameters_general_count = [ \n",
    "    {\n",
    "        'countvectorizer__min_df': [1,3,5], \n",
    "        'countvectorizer__stop_words': [None, stop],\n",
    "        'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "9b70fc442a67aa99751c7dafa65a211d5828d547"
   },
   "outputs": [],
   "source": [
    "parameters_of_svc_tfidf = [ \n",
    "    {\n",
    "        'LinearSVC__C': [0.01, 0.1, 1, 10, 100], \n",
    "        'tfidfvectorizer__min_df': [1,3,5], \n",
    "        'tfidfvectorizer__stop_words': [stop],\n",
    "        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    } \n",
    "]\n",
    "\n",
    "parameters_of_general_tfidf = [ \n",
    "    {\n",
    "        'tfidfvectorizer__min_df': [1,3,5], \n",
    "        'tfidfvectorizer__stop_words': [stop],\n",
    "        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "4b267cc206e5660c3d5db3ebeacf7c2f23b95467"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca2944ff5ef7eb35929771cf5a32b2070ed63c55"
   },
   "source": [
    "Şimdi modellerimizi çalıştırmaya başlayalım. İlk olarak CountVectorizer içeren modeller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "2aa04f6210294ba4842f48747f138e0f459f6810"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ismi: LinearSVC\n",
      "En iyi cross-validation score: 89.24\n",
      "En iyi parametreler:  {'LinearSVC__C': 1, 'countvectorizer__min_df': 1, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']}\n",
      "[[1118    0]\n",
      " [   0 1132]]\n",
      "Test score: 89.21%\n",
      "--------------------------\n",
      "Model ismi: Multinomial NB\n",
      "En iyi cross-validation score: 87.20\n",
      "En iyi parametreler:  {'countvectorizer__min_df': 1, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': None}\n",
      "[[1102   16]\n",
      " [   3 1129]]\n",
      "Test score: 86.42%\n",
      "--------------------------\n",
      "Model ismi: Decision Tree\n",
      "En iyi cross-validation score: 85.47\n",
      "En iyi parametreler:  {'countvectorizer__min_df': 1, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__stop_words': ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']}\n",
      "[[1118    0]\n",
      " [   0 1132]]\n",
      "Test score: 89.61%\n",
      "--------------------------\n",
      "Model ismi: Random Forest\n",
      "En iyi cross-validation score: 86.18\n",
      "En iyi parametreler:  {'countvectorizer__min_df': 3, 'countvectorizer__ngram_range': (1, 3), 'countvectorizer__stop_words': ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']}\n",
      "[[1116    2]\n",
      " [  12 1120]]\n",
      "Test score: 87.48%\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for models, parameters, name in zip([LinearSVC_count, Naive_count, Decision_count, RandomForest_count],\n",
    "                                    [parameters_of_svc_count, parameters_general_count, parameters_general_count, parameters_general_count],\n",
    "                                    [\"LinearSVC\",\"Multinomial NB\",\"Decision Tree\",\"Random Forest\"]):\n",
    "\n",
    "    grid = GridSearchCV(models, parameters, cv=5)\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(\"Model ismi: \"+ name)\n",
    "    print(\"En iyi cross-validation score: {:.2f}\".format(grid.best_score_ * 100))\n",
    "    print(\"En iyi parametreler: \", grid.best_params_)\n",
    "    \n",
    "    y_train_pred = grid.predict(x_train)\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "    \n",
    "    final_model = grid.best_estimator_\n",
    "    final_test_prediction = final_model.score(x_test, y_test)\n",
    "    print(\"Test score: {:.2f}%\".format(final_test_prediction * 100))    \n",
    "    print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2bd2e0264bc31e885c7765159cb6ebd733364df7"
   },
   "source": [
    "Şimdi de TFIDF vectorizer'ına göre sonuç alalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "1d25390e94c03e31a4fc06ccd82d80f657c90c67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ismi: LinearSVC\n",
      "En iyi cross-validation score: 88.44\n",
      "En iyi parametreler:  {'LinearSVC__C': 10, 'tfidfvectorizer__min_df': 1, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__stop_words': ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']}\n",
      "[[1118    0]\n",
      " [   0 1132]]\n",
      "Test score: 90.15%\n",
      "--------------------------\n",
      "Model ismi: Multinomial Naive Bayes\n",
      "En iyi cross-validation score: 86.18\n",
      "En iyi parametreler:  {'tfidfvectorizer__min_df': 1, 'tfidfvectorizer__ngram_range': (1, 3), 'tfidfvectorizer__stop_words': ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']}\n",
      "[[1118    0]\n",
      " [   0 1132]]\n",
      "Test score: 86.95%\n",
      "--------------------------\n",
      "Model ismi: Decision Tree\n",
      "En iyi cross-validation score: 83.24\n",
      "En iyi parametreler:  {'tfidfvectorizer__min_df': 1, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__stop_words': ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']}\n",
      "[[1118    0]\n",
      " [   0 1132]]\n",
      "Test score: 84.55%\n",
      "--------------------------\n",
      "Model ismi: Random Forest\n",
      "En iyi cross-validation score: 85.42\n",
      "En iyi parametreler:  {'tfidfvectorizer__min_df': 3, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__stop_words': ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani']}\n",
      "[[1114    4]\n",
      " [  13 1119]]\n",
      "Test score: 87.48%\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for models, parameters, name in zip([LinearSVC_tfidf, Naive_tfidf, Decision_tfidf, RandomForest_tfidf],\n",
    "                                    [parameters_of_svc_tfidf, parameters_of_general_tfidf, parameters_of_general_tfidf, parameters_of_general_tfidf],\n",
    "                                    [\"LinearSVC\",\"Multinomial Naive Bayes\",\"Decision Tree\",\"Random Forest\"]):\n",
    "\n",
    "    grid = GridSearchCV(models, parameters, cv=5)\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(\"Model ismi: \"+ name)\n",
    "    print(\"En iyi cross-validation score: {:.2f}\".format(grid.best_score_ * 100))\n",
    "    print(\"En iyi parametreler: \", grid.best_params_)\n",
    "    \n",
    "    y_train_pred = grid.predict(x_train)\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "    \n",
    "    final_model = grid.best_estimator_\n",
    "    final_test_prediction = final_model.score(x_test, y_test)\n",
    "    print(\"Test score: {:.2f}%\".format(final_test_prediction * 100))    \n",
    "    print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c801ef2ed54a617ce309b3cc206896b8ddb87770"
   },
   "source": [
    "En iyi test score'u veren modelimiz TFIDF vectorizer'ı ile LinearSVC modeli oldu. En iyi parametreler: C=10, min_df=1, stop_words=word, ngram_range=(1,2). Score: %90.15\n",
    "\n",
    "En iyi sonuç aldığımız modeli parametreleri ve vectorizer'ıyla birlikte yeni bir pipeline yapısında tutalım. Yeni gelen değerler direk bu yeni yarattığımız yapıya gidecek ve orada kontrol ettirilecek.\n",
    "\n",
    "En iyi sonuç aldığımız modelimizin pipeline yapısı:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "82b5ca17b3273c94446eeb4c4020d52fad3656f1"
   },
   "outputs": [],
   "source": [
    "# En iyi sonuç veren parametrelerimizi de yapının içerisine ekliyoruz.\n",
    "best_pipeline = Pipeline([\n",
    "    ('tfidfvectorizer', TfidfVectorizer(min_df=1,stop_words=stop,ngram_range=(1,2))),\n",
    "    ('LinearSVC', LinearSVC(C=10,max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "a847166cfcf73dde57e03a6557dd442a68ce8059"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "e4b75a106727d3de32dc16787fe69a5928b6a028"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tfidfvectorizer',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani'],\n",
       "          strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('LinearSVC',\n",
       "  LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "       multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "       verbose=0))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "00db3b6e73ab8f7e27fee2fde75009ca8003e7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test score: 0.9014647137150466\n",
      "Confusion matrix\n",
      " [[335  45]\n",
      " [ 29 342]]\n"
     ]
    }
   ],
   "source": [
    "final_test_prediction = best_pipeline.score(x_test,y_test)\n",
    "print(\"Final test score:\",final_test_prediction)\n",
    "\n",
    "y_test_pred = best_pipeline.predict(x_test)\n",
    "print(\"Confusion matrix\\n\",confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "648a0fa1e3a9cb0c90a6a5bfa8516a970eb2fda5"
   },
   "source": [
    "Bu modelimizi bir dosya içerisine save etmemiz gerekiyor şu an. Dosyanın adını en 'latest_model' yapacağım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "b3417e3bc2113e339bf5e8cde154b96f13854f6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latest_model.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(best_pipeline,\"latest_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc1a6f7933e3371f824956eb7dcc36324b07fd9c"
   },
   "source": [
    "Dosyayı load ederek deneyelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "0c7e665268500c6d1dd89caa727a88506bc9349a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Score: 0.9014647137150466\n"
     ]
    }
   ],
   "source": [
    "best_model = joblib.load(\"latest_model.pkl\")\n",
    "final_test_prediction = best_model.score(x_test,y_test)\n",
    "print(\"Final Test Score:\",final_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc2fefebb696dc1d41ba24de3a775b5020f5e855"
   },
   "source": [
    "Gördüğümüz gibi; dosyamızın içinde bulunan final score'u önümüze geldi!\n",
    "\n",
    "Şimdi kendi yazdığımız bir kaç örnekle classification yapabiliyor muyuz test edelim!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "53666d0c84751ca4e6e035d64a7f100d53eef211"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.predict(['gayet iyi','salak','bu ne amk','güzelsin','siktir','hava çok güzel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f46753662998d06461e05465e2f6c591825b85cc"
   },
   "source": [
    "Gördüğünüz gibi hakaret içeren ifadeler 1 nolu (hakaret içeren ifadeler) hakaret içermeyen ifadeler 0 nolu (hakaret edilmeyen) class'ımıza classify yapılmış oldu. Modelimiz gayet sağlıklı çalışıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e99d140c53456999831d629b4b1906c75f5c7ef"
   },
   "source": [
    "**SONUÇ**\n",
    "* Modelimizi en iyi şekilde eğittik ve bir cümle içerisinde hakaret olup olmadığını bulan bir model geliştirdik. Son bölümde yaptığımız testlerde de açıkça görüldüğü gibi; modelimiz gayet güzel şekilde çalışıyor. Predict ederken kullandığım kaba-saba ve küfürlü ifadeler için sizden özür dilerim ancak modelimi test etmem için zorunluydu bu :) Umarım siz de benim kadar çok şey öğrenmişsinizdir. Yorumlarınız benim için çok kıymetli, lütfen yorum yapmaya çekinmeyin!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
